# Lesson 01: ML Foundations for Alignment

## Overview
This lesson builds the minimal ML foundation needed for alignment: supervised learning, loss functions, generalization, and basic optimization. We also introduce probabilistic notation used throughout the course.

## Learning objectives
- Define supervised learning, datasets, hypotheses, and loss functions.
- Explain train/val/test splits and generalization intuition.
- Apply basic gradient descent and SGD intuition.
- Read probabilistic notation (expectation, distributions) used in RL.

## Lesson structure
1. Why ML foundations matter for alignment
   - Where policies and objectives come from
   - How losses connect to behavior
2. Supervised learning setup
   - Dataset, hypothesis class, empirical risk
   - Common losses (MSE, cross-entropy)
3. Generalization and evaluation
   - Train/val/test, overfitting, bias-variance intuition
4. Optimization basics
   - Gradient descent and SGD
   - Learning rate, regularization, early stopping
5. Probabilistic notation primer
   - Random variables, distributions, expectation
   - How this connects to RL expectations later

## Key terms
- supervised learning, loss, risk, generalization, SGD, expectation

## Suggested prep
- None required; optional refresher on basic calculus.
