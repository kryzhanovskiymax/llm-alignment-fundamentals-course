# Lesson 05: RL Fundamentals (Aligned to Book)

## Overview
This lesson mirrors the book's foundation chapters. We cover probability and estimation tools, then formalize the MDP setup, trajectories, returns, occupancy measures, and value functions.

## Learning objectives
- Use expectation, Monte-Carlo estimation, and importance sampling in RL contexts.
- Apply the log-derivative trick and understand why baselines reduce variance.
- Define an MDP, trajectory distribution, and discounted return objective.
- Explain value functions, Bellman expectation equations, and advantages.

## Lesson structure (aligned to book)
### 1. Important Concepts
- Probability model and basic notation
- Monte-Carlo estimation
- Importance Sampling (IS)
- Log-derivative trick (score-function identity)
- Temporal-Difference (TD) idea

### 2. Reinforcement Learning Fundamentals
- MDP problem setup
  - Policy
  - Trajectory and trajectory distribution
  - Return and optimization objective
- Important constructions
  - Discounted occupancy measures
- Value functions
  - Definitions: V^pi, Q^pi, A^pi
  - Connection between V^pi and Q^pi
  - Bellman expectation equations for V^pi and Q^pi
  - Advantage function properties

## Key terms
- expectation, Monte-Carlo, importance sampling, MDP, trajectory, return, occupancy, value, advantage

## Suggested prep
- Skim the book sections "Important Concepts" and "Reinforcement Learning Fundamentals".
