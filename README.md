# Математические основы обучения с подкреплением для выравнивания больших языковых моделей

Этот репозиторий содержит курс из 12 уроков, который закладывает математический фундамент, необходимый для понимания и реализации современных методов выравнивания больших языковых моделей (LLM) на основе обучения с подкреплением.

**Общая структура**
- **Уроки 1–4:** Введение в ML, DL и LLM (достаточно для понимания, где применяется RL/alignment и что такое "политика").
- **Уроки 5–8:** Основная математическая база RL + оптимизация политики, доминирующая в современных методах alignment'a (эти уроки соответствуют прилагаемому черновику книги).
- **Уроки 9–10:** Практические аспекты: инструменты/системы и оценка/бенчмарки.
- **Уроки 11–12:** Сессии reading group (темы выбираются динамически).

---

## Роадмап курса

| Номер урока | Название урока | Описание урока | Ссылка на слайды | Ссылка на конспект |
|---:|---|---|---|---|
| 1 | [Основы ML для alignment'a](lesson_01/README.md) | Настройка обучения с учителем, функции потерь, train/val/test, интуиция обобщения, базовая оптимизация (SGD), вероятностное обоснование, используемого в курсе. | [Слайды](lesson_01/slides/) | [Конспект](lesson_01/conspect.md) |
| 2 | [Основы глубокого обучения](lesson_02/README.md) | Нейронные сети, обратное распространение, инициализация, регуляризация, нормализация, интуиция законов масштабирования (высокий уровень), почему градиенты/дисперсия важны для RL. | [Слайды](lesson_02/slides/) | [Конспект](lesson_02/conspect.md) |
| 3 | [Моделирование последовательностей + трансформеры](lesson_03/README.md) | Авторегрессивное моделирование, целевая функция кросс-энтропии, attention/трансформеры, токенизация, основы декодирования/сэмплирования, необходимые для представления LLM как политик. | [Слайды](lesson_03/slides/) | [Конспект](lesson_03/conspect.md) |
| 4 | [Обзор pipeline обучения LLM](lesson_04/README.md) | Предобучение -> SFT -> данные предпочтений/моделирование вознаграждений -> техники RL-alignment'a; дизайн вознаграждений для данных предпочтений; проблемы alignment и почему они требуют применения RL. | [Слайды](lesson_04/slides/) | [Конспект](lesson_04/conspect.md) |
| 5 | [Основы RL](lesson_05/README.md) | Обзор вероятности/ожидания, оценка Монте-Карло, основы importance sampling, MDP, траектории, returns, дисконтированная occupancy, функции ценности, идея TD. | [Слайды](lesson_05/slides/) | [Конспект](lesson_05/conspect.md) |
| 6 | [Методы градиента политики](lesson_06/README.md) | Трюк с логарифмической производной, REINFORCE, reward-to-go, baselines/контрольные вариаты, actor-critic (A2C/A3C), оценка advantage как снижение дисперсии. | [Слайды](lesson_06/slides/) | [Конспект](lesson_06/conspect.md) |
| 7 | [Методы ограниченной оптимизации](lesson_07/README.md) | Лемма о разности производительности, локальная суррогатная целевая функция, штраф за сдвиг распределения, консервативные обновления политики (CPI), доверительные области и ограничения KL, механика TRPO. | [Слайды](lesson_07/slides/) | [Конспект](lesson_07/conspect.md) |
| 8 | [Эффективные современные методы выравнивания](lesson_08/README.md) | Формализация LLM-как-MDP (токены/действия), целевые функции с KL-регуляризацией vs референсная модель, клиппинг в стиле PPO, baseline без критика/групповые baselines, GRPO + варианты (DrGRPO/DAPO), альтернативы на уровне последовательности (GSPO), мягкое гейтирование (SAPO). | [Слайды](lesson_08/slides/) | [Конспект](lesson_08/conspect.md) |
| 9 | [Практические инструменты и системы выравнивания](lesson_09/README.md) | Практический pipeline: данные -> сэмплирование -> оценка/вознаграждение -> оптимизация; общие паттерны инструментов (циклы обучения, движки rollout типа vLLM, распределенное выполнение, LoRA/PEFT), логирование/мониторинг, воспроизводимость. | [Слайды](lesson_09/slides/) | [Конспект](lesson_09/conspect.md) |
| 10 | [Бенчмарки и оценка для выровненных LLM](lesson_10/README.md) | Offline eval vs online, автоматическая vs человеческая оценка, типы бенчмарков (следование инструкциям, безопасность, рассуждения), валидация моделей предпочтений, проверки на взлом вознаграждений, регрессионное тестирование. | [Слайды](lesson_10/slides/) | [Конспект](lesson_10/conspect.md) |
| 11 | [Reading Group I](lesson_11/README.md) | (Сессия reading group; статьи/темы будут определены позже.) | [Слайды](lesson_11/slides/) | [Конспект](lesson_11/conspect.md) |
| 12 | [Reading Group II](lesson_12/README.md) | (Сессия reading group; статьи/темы будут определены позже.) | [Слайды](lesson_12/slides/) | [Конспект](lesson_12/conspect.md) |

---

## Исходники книги (Уроки 5–8)

Черновик книги *"RL Foundations for LLM Alignment"* находится в:

- `materials/book/src.tex` — исходник LaTeX
- `materials/book/LLM_alignment_book.pdf` — скомпилированный PDF (опционально, но рекомендуется)

Уроки **5–8** разработаны в соответствии с главами книги:
- **L5:** основы теории вероятностей + основы RL
- **L6:** градиенты политики и actor–critic
- **L7:** ограниченная оптимизация (теория CPI/TRPO/PPO)
- **L8:** современные методы выравнивания LLM (в стиле PPO/GRPO и дальше)

---

## Рекомендуемые "минимальные требования" для каждой папки урока

Каждая папка `lesson_XX/` в идеале должна содержать:
- `README.md` (цели, план, чтение, домашнее задание)
- `slides/`
- `conspect.md` (компактные математические заметки)

Опционально:
- `exercises/` (наборы задач)
- `notebooks/` (демонстрационные реализации)
- `reading/` (список статей + аннотации)
